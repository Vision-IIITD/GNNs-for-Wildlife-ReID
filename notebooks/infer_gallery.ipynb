{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/wildlife10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from typing import Tuple, List\n",
    "\n",
    "sys.path.append(os.path.join(os.getcwd(), \"LightGlue\"))\n",
    "sys.path.append(os.path.join(os.getcwd(), \"SuperGluePretrainedNetwork\"))\n",
    "sys.path.append(os.path.join(os.getcwd(), \"omniglue\"))\n",
    "sys.path.append(os.path.join(os.getcwd(), \"wildlife-tools\"))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import timm\n",
    "import torchvision.transforms as T\n",
    "from argparse import ArgumentParser\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "from wildlife_datasets import analysis, datasets, loader, splits\n",
    "from wildlife_tools.features import DeepFeatures\n",
    "from wildlife_tools.data import WildlifeDataset \n",
    "from wildlife_tools.similarity import CosineSimilarity\n",
    "\n",
    "from LightGlue.lightglue import LightGlue, SuperPoint, SIFT\n",
    "from LightGlue.lightglue.utils import load_image, rbd\n",
    "from LightGlue.lightglue import viz2d\n",
    "\n",
    "from SuperGluePretrainedNetwork import match_pairs as SuperGlueMatching\n",
    "\n",
    "# from omniglue import omniglue_extract as omniglue\n",
    "\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "memo_gallery = []\n",
    "memo_query = []\n",
    "detector = None\n",
    "MATCHER = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(dataset_name: str) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, str]:\n",
    "    \"\"\"\n",
    "    Load dataset and split into training, validation, and test sets.\n",
    "\n",
    "    Args:\n",
    "        dataset_name (str): Name of the dataset.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, str]: DataFrames for training, validation, and test sets, and the dataset root directory.\n",
    "    \"\"\"\n",
    "    if dataset_name == \"LeopardID2022\":\n",
    "        d = datasets.LeopardID2022(\"datasets/LeopardID2022\")\n",
    "    elif dataset_name == \"HyenaID2022\":\n",
    "        d = datasets.HyenaID2022(\"datasets/HyenaID2022\")\n",
    "    elif dataset_name == \"ATRW\":\n",
    "        d = datasets.ATRW(\"datasets/ATRW\")\n",
    "    elif dataset_name == \"WII\":\n",
    "        import wii_dataset\n",
    "        d = wii_dataset.WII(\"datasets/WII\")\n",
    "    else:\n",
    "        raise ValueError(\"Invalid dataset name\")\n",
    "    \n",
    "    # remove those samples where an identity has fewer than 5 samples\n",
    "    d.df = d.df.groupby(\"identity\").filter(lambda x: len(x) >= 5)\n",
    "\n",
    "    if dataset_name == \"ATRW\":\n",
    "        df_train = d.df[d.df[\"original_split\"] == \"train\"]\n",
    "        df_test = d.df[d.df[\"original_split\"] == \"test\"]\n",
    "        df_val = df_test\n",
    "\n",
    "    elif dataset_name == \"WII\":\n",
    "        df_train = d.df[d.df[\"split\"] == \"train\"]\n",
    "        df_test = d.df[d.df[\"split\"] == \"test\"]\n",
    "\n",
    "        # remove those entries that dont have a flank column\n",
    "        df_test = df_test.dropna(subset=[\"flank\"])\n",
    "        df_test = df_test.dropna(subset=[\"gallery_split\"])\n",
    "\n",
    "        df_val = df_test\n",
    "\n",
    "    else:\n",
    "        n_identites = len(d.df['identity'].unique())\n",
    "\n",
    "        n_test_ids = int(np.ceil(0.33 * n_identites))\n",
    "        n_val_ids = int(np.ceil(0.2 * (n_identites - n_test_ids)))\n",
    "\n",
    "        splitter = splits.DisjointSetSplit(n_class_test=n_test_ids) # 64 test IDs = ceil(0.33 * 193)\n",
    "        # splitter = splits.ClosedSetSplit(0.67)\n",
    "        for idx_train, idx_test in splitter.split(d.df):\n",
    "            _df_train, df_test = d.df.loc[idx_train], d.df.loc[idx_test]\n",
    "\n",
    "        splitter_2 = splits.DisjointSetSplit(n_class_test=n_val_ids) # 26 val IDs = ceil(0.2 * 129)\n",
    "        for idx_train, idx_val in splitter_2.split(_df_train):\n",
    "            df_train, df_val = _df_train.loc[idx_train], _df_train.loc[idx_val]\n",
    "\n",
    "\n",
    "    df_train = df_train.groupby(\"identity\").filter(lambda x: len(x) >= 5)\n",
    "    df_test = df_test.groupby(\"identity\").filter(lambda x: len(x) >= 5)\n",
    "    df_val = df_val.groupby(\"identity\").filter(lambda x: len(x) >= 5)\n",
    "\n",
    "    return df_train, df_val, df_test, d.root\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_gallery_split(df: pd.DataFrame, root: str, transform: T.Compose, dataset_name: str) -> Tuple[WildlifeDataset, WildlifeDataset]:\n",
    "    \"\"\"\n",
    "    Split dataset into query and gallery sets. For each identity, the first sample is used as the gallery image and the rest are used as query images.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing dataset information.\n",
    "        root (str): Root directory of the dataset.\n",
    "        transform (T.Compose): Transformations to apply to the images.\n",
    "        dataset_name (str): Name of the dataset.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[WildlifeDataset, WildlifeDataset]: Query and gallery datasets.\n",
    "    \"\"\"\n",
    "\n",
    "    if dataset_name == \"WII\":\n",
    "        df_left = df[df[\"flank\"] == \"left\"]\n",
    "        df_right = df[df[\"flank\"] == \"right\"]\n",
    "\n",
    "        df_left_query = df_left[df_left[\"gallery_split\"] == \"query\"]\n",
    "        df_left_gallery = df_left[df_left[\"gallery_split\"] == \"gallery\"]\n",
    "\n",
    "        df_right_query = df_right[df_right[\"gallery_split\"] == \"query\"]\n",
    "        df_right_gallery = df_right[df_right[\"gallery_split\"] == \"gallery\"]\n",
    "\n",
    "        dataset_left_query = WildlifeDataset(df_left_query, root, transform=transform, img_load=\"bbox\")\n",
    "        dataset_left_gallery = WildlifeDataset(df_left_gallery, root, transform=transform, img_load=\"bbox\")\n",
    "\n",
    "        dataset_right_query = WildlifeDataset(df_right_query, root, transform=transform, img_load=\"bbox\")\n",
    "        dataset_right_gallery = WildlifeDataset(df_right_gallery, root, transform=transform, img_load=\"bbox\")\n",
    "\n",
    "        return dataset_left_query, dataset_left_gallery, dataset_right_query, dataset_right_gallery\n",
    "    else:\n",
    "        df_query = df.groupby(\"identity\").apply(lambda x: x.iloc[1:])\n",
    "        df_gallery = df.groupby(\"identity\").apply(lambda x: x.iloc[:1])\n",
    "\n",
    "    if dataset_name == \"ATRW\":\n",
    "        dataset_query = WildlifeDataset(df_query, root, transform=transform)\n",
    "        dataset_gallery = WildlifeDataset(df_gallery, root, transform=transform)\n",
    "    else:    \n",
    "        dataset_query = WildlifeDataset(df_query, root, transform=transform, img_load=\"bbox\")\n",
    "        dataset_gallery = WildlifeDataset(df_gallery, root, transform=transform, img_load=\"bbox\")\n",
    "    return dataset_query, dataset_gallery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matches_megadesc(dataset_query: WildlifeDataset, dataset_gallery: WildlifeDataset, device: torch.device) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Get top 5 matches using MegaDescriptor.\n",
    "\n",
    "    Args:\n",
    "        dataset_query (WildlifeDataset): Query image dataset.\n",
    "        dataset_gallery (WildlifeDataset): Gallery image dataset.\n",
    "        device (torch.device): Device to run the model on.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray (n_query, 5): Indices of the top 5 matches for each query image.\n",
    "    \"\"\"\n",
    "    # Load the MegaDepth model\n",
    "    model = timm.create_model(\"hf-hub:BVRA/MegaDescriptor-L-384\", pretrained=True)\n",
    "    model = model.eval().to(device)\n",
    "\n",
    "    # Load the MegaDepth descriptors\n",
    "    features = DeepFeatures(model, device=device)\n",
    "\n",
    "    # Compute the MegaDepth descriptors for the query and gallery images\n",
    "    query_desc = features(dataset_query)\n",
    "    gallery_desc = features(dataset_gallery)\n",
    "\n",
    "    # dataset[2] is the flank. \n",
    "\n",
    "    # Compute the cosine similarity between the query and gallery descriptors\n",
    "    similarity = CosineSimilarity()\n",
    "    similarity_matrix = similarity(query_desc, gallery_desc)['cosine']\n",
    "\n",
    "    # Find the top 5 matches\n",
    "    # knn = KnnClassifier(k=1)\n",
    "    # matches = knn(similarity_matrix)\n",
    "    top_k = 5\n",
    "    matches = np.argsort(similarity_matrix, axis=1)[:, -top_k:][:, ::-1]  # Sort and take top 5, then reverse order\n",
    "\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matches_lightglue(feats0, feats1, device: torch.device, detector: str) -> Tuple[np.ndarray, np.ndarray, dict]:\n",
    "    \"\"\"\n",
    "    Get matches using LightGlue.\n",
    "\n",
    "    Args:\n",
    "        image0 (torch.Tensor): First image tensor.\n",
    "        image1 (torch.Tensor): Second image tensor.\n",
    "        device (torch.device): Device to run the model on.\n",
    "        detector (str): Keypoint detector to use.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[np.ndarray, np.ndarray, dict]: Points from the first and second images and matches.\n",
    "    \"\"\"\n",
    "    global MATCHER\n",
    "    # if detector == \"superpoint\":\n",
    "    #     extractor = SuperPoint(max_num_keypoints=2048).eval().cuda()  # load the extractor\n",
    "    # elif detector == \"sift\":\n",
    "    #     extractor = SIFT(max_num_keypoints=2048).eval().cuda()\n",
    "    # else:\n",
    "    #     raise ValueError(f\"Invalid keypoint detector: {detector}\")\n",
    "    \n",
    "    # MATCHER = LightGlue(features=detector).eval().cuda()  # load the matcher\n",
    "\n",
    "    # image0 = image0.to(device)\n",
    "    # image1 = image1.to(device)\n",
    "\n",
    "    # feats0 = extractor.extract(image0)\n",
    "    # feats1 = extractor.extract(image1)\n",
    "\n",
    "    # match the features\n",
    "    matches01 = MATCHER({'image0': feats0, 'image1': feats1})\n",
    "    feats0, feats1, matches01 = [rbd(x) for x in [feats0, feats1, matches01]]  # remove batch dimension\n",
    "    matches = matches01['matches']  # indices with shape (K,2)\n",
    "    points0 = feats0['keypoints'][matches[..., 0]]  # coordinates in image #0, shape (K,2)\n",
    "    points1 = feats1['keypoints'][matches[..., 1]]  # coordinates in image #1, shape (K,2)\n",
    "\n",
    "    indices = matches01['scores'] > 0.05\n",
    "    points0, points1 = points0[indices], points1[indices]\n",
    "    matches01['matches'] = matches01['matches'][indices]\n",
    "    matches01['scores'] = matches01['scores'][indices]\n",
    "\n",
    "    # Visualization\n",
    "    # image0 = image0.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
    "    # image1 = image1.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
    "    # axes = viz2d.plot_images([image0, image1])\n",
    "    # viz2d.plot_matches(points0, points1, color=\"lime\", lw=0.2)\n",
    "    # viz2d.add_text(0, f'Stop after {matches01[\"stop\"]} layers', fs=20)\n",
    "\n",
    "    # kpc0, kpc1 = viz2d.cm_prune(matches01[\"prune0\"]), viz2d.cm_prune(matches01[\"prune1\"])\n",
    "    # viz2d.plot_images([image0, image1])\n",
    "    # viz2d.plot_keypoints([points0, points1], colors=[kpc0, kpc1], ps=10)\n",
    "\n",
    "    return points0, points1, matches01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matches_superglue(image0: torch.Tensor, image1: torch.Tensor, device: torch.device, detector: str) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Get matches using SuperGlue.\n",
    "\n",
    "    Args:\n",
    "        image0 (torch.Tensor): First image tensor.\n",
    "        image1 (torch.Tensor): Second image tensor.\n",
    "        device (torch.device): Device to run the model on.\n",
    "        detector (str): Keypoint detector to use.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[np.ndarray, np.ndarray, np.ndarray]: Points from the first and second images and match scores.\n",
    "    \"\"\"\n",
    "    if detector == \"superpoint\":\n",
    "        extractor = SuperPoint(max_num_keypoints=2048).eval().cuda()  # load the extractor\n",
    "    # elif detector == \"sift\":\n",
    "    #     extractor = SIFT(max_num_keypoints=2048).eval().cuda()\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid keypoint detector: {detector}\")\n",
    "\n",
    "\n",
    "    image0 = image0.to(device)\n",
    "    image1 = image1.to(device)\n",
    "\n",
    "    # import pdb; pdb.set_trace()\n",
    "    feats0 = extractor.extract(image0)\n",
    "    feats1 = extractor.extract(image1)\n",
    "\n",
    "    # Remove the batch dimension and convert to NumPy array\n",
    "    image0 = image0.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
    "    image0 = (image0 * 255).astype(np.uint8)\n",
    "    image0 = cv2.cvtColor(image0, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    image1 = image1.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
    "    image1 = (image1 * 255).astype(np.uint8)\n",
    "    image1 = cv2.cvtColor(image1, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    sys.stdout = open(os.devnull, \"w\")\n",
    "    points0, points1, match_scores = SuperGlueMatching.run(img0=image0, kpts0=feats0, img1=image1, kpts1=feats1, match_thresold=0.2)\n",
    "    sys.stdout = sys.__stdout__\n",
    "    return points0, points1, match_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_arcface(image0: torch.Tensor, device: torch.device) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Run ArcFace on the input images.\n",
    "\n",
    "    Args:\n",
    "        image0 (torch.Tensor): First image tensor. (1, C, H, W) format. Image resized to 112x112, normalized to [-1, 1].\n",
    "        device (torch.device): Device to run the model on.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Cosine similarity between the embeddings.\n",
    "    \"\"\"\n",
    "    sys.path.append(os.path.join(os.path.dirname(__file__), \"insightface/recognition/arcface_torch/\"))\n",
    "    from backbones import get_model\n",
    "\n",
    "    weights_path = \"/home/atharv21027/ReID-with-graphs/insightface/recognition/arcface_torch/work_dirs/ms1mv3_r50_onegpu/model.pt\"\n",
    "    model = get_model(\"r50\", fp16=False)\n",
    "    model.load_state_dict(torch.load(weights_path))\n",
    "    model = model.eval().to(device)\n",
    "    image0 = image0.to(device)\n",
    "    with torch.no_grad():\n",
    "        embedding = model(image0)\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(dataset_query: WildlifeDataset, dataset_gallery: WildlifeDataset, device: torch.device, detector: str, matcher: str, memo_gallery, memo_query) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Inference pipeline for feature matching.\n",
    "    Computes the top 5 matches for each query image and computes the top-1, top-3, and top-5 accuracy.\n",
    "\n",
    "    Args:\n",
    "        dataset_query (WildlifeDataset): Query image dataset.\n",
    "        dataset_gallery (WildlifeDataset): Gallery image dataset.\n",
    "        device (torch.device): Device to run the model on.\n",
    "        detector (str): Keypoint detector to use.\n",
    "        matcher (str): Feature matcher to use.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[np.ndarray, np.ndarray]\n",
    "            - np.ndarray: Predictions (N, 5)\n",
    "            - np.ndarray: Ground truth (N,)\n",
    "    \"\"\"\n",
    "\n",
    "    global MATCHER\n",
    "    # memoize\n",
    "    \n",
    "\n",
    "\n",
    "    if matcher == \"megadescriptor\":\n",
    "        top_5_matches = get_matches_megadesc(dataset_query, dataset_gallery, device)\n",
    "        predictions = np.array(top_5_matches)\n",
    "        gt = np.array([dataset_query[i][1] for i in range(len(dataset_query))])\n",
    "        return predictions, gt\n",
    "\n",
    "    if matcher == \"omniglue\":\n",
    "            og = omniglue.OmniGlue(\n",
    "            og_export='omniglue/models/og_export',\n",
    "            sp_export='omniglue/models/sp_v6',\n",
    "            dino_export='omniglue/models/dinov2_vitb14_pretrain.pth',\n",
    "        )\n",
    "\n",
    "    if matcher == \"lightglue\":\n",
    "        if detector == \"superpoint\":\n",
    "            extractor = SuperPoint(max_num_keypoints=2048).eval().cuda()  # load the extractor\n",
    "        elif detector == \"sift\":\n",
    "            extractor = SIFT(max_num_keypoints=2048).eval().cuda()\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid keypoint detector: {detector}\")\n",
    "\n",
    "        MATCHER = LightGlue(features=detector).eval().cuda()\n",
    "    for i in tqdm(range(len(dataset_gallery)), desc=\"Memoizing gallery\"):\n",
    "        memo_gallery.append(extractor.extract(dataset_gallery[i][0].unsqueeze(0).to(device)))\n",
    "    for i in tqdm(range(len(dataset_query)), desc=\"Memoizing query\"):\n",
    "        memo_query.append(extractor.extract(dataset_query[i][0].unsqueeze(0).to(device)))\n",
    "            \n",
    "    if detector == \"arcface\" or matcher == \"arcface\":\n",
    "        sys.path.append(os.path.join(os.getcwd(), \"insightface/recognition/arcface_torch/\"))\n",
    "        from backbones import get_model\n",
    "\n",
    "        weights_path = \"/home/atharv21027/ReID-with-graphs/insightface/recognition/arcface_torch/work_dirs/ms1mv3_r50_onegpu/model.pt\"\n",
    "        arcface = get_model(\"r50\", fp16=False)\n",
    "        arcface.load_state_dict(torch.load(weights_path))\n",
    "        arcface = arcface.eval().to(device)\n",
    "\n",
    "    predictions = np.zeros((len(dataset_query), 5)) # store the indices of the top 5 IDs (decreasing order of matches)\n",
    "    gt = np.zeros(len(dataset_query)) # store the ground truth IDs\n",
    "\n",
    "    # store number of left flanks in the gallery\n",
    "    # n_left_flanks = np.sum([dataset_gallery[i][2] == \"left\" for i in range(len(dataset_gallery))])\n",
    "    # n_right_flanks = np.sum([dataset_gallery[i][2] == \"right\" for i in range(len(dataset_gallery))])\n",
    "    n_left_flanks = len(dataset_gallery); n_right_flanks = 0\n",
    "\n",
    "    for i in tqdm(range(len(dataset_query)), desc=\"Querying\"):\n",
    "        # image_query = dataset_query[i][0]\n",
    "        # image_query = image_query.unsqueeze(0).to(device)\n",
    "        feat_query = memo_query[i]\n",
    "        # query_flank = dataset_query[i][2]\n",
    "\n",
    "        # for each image in dataset_gallery, store the number of matches\n",
    "        n_matches_list = np.zeros(n_left_flanks + n_right_flanks) # store the number of matches for each image in the gallery\n",
    "\n",
    "        if detector == \"arcface\" or matcher == \"arcface\":\n",
    "            query_emb = arcface(image_query)\n",
    "\n",
    "        # for j, (image_gallery, label, gallery_flank) in enumerate(dataset_gallery):\n",
    "        # for j, (image_gallery, label) in enumerate(dataset_gallery):\n",
    "        for j, feat_gallery in enumerate(memo_gallery):\n",
    "            # if (gallery_flank != query_flank): \n",
    "            #     # Invalid matching\n",
    "            #     n_matches_list[j] = -1\n",
    "            #     continue\n",
    "\n",
    "            # image_gallery = image_gallery.unsqueeze(0).to(device)\n",
    "\n",
    "            if matcher == \"lightglue\":\n",
    "                pts_query, pts_gallery, matches = get_matches_lightglue(feat_query, feat_gallery, device, detector=detector)\n",
    "                n_matches = len(matches[\"matches\"])\n",
    "            elif matcher == \"superglue\":\n",
    "                pts_query, pts_gallery, matches = get_matches_superglue(image_query, image_gallery, device, detector=detector)\n",
    "                n_matches = len(matches)\n",
    "            elif matcher == \"omniglue\":\n",
    "                pts_query, pts_gallery, matches = get_matches_omniglue(image_query, image_gallery, device, detector=detector, omniglue_matcher=og)\n",
    "                n_matches = len(matches)\n",
    "            # elif matcher == \"megadescriptor\":\n",
    "                # n_matches = get_matches_megadesc(image_query, image_gallery, device)\n",
    "            elif matcher == \"arcface\":\n",
    "                gallery_emb = arcface(image_gallery)\n",
    "                # not appropriate naming, just for convenience of writing less code\n",
    "                n_matches = torch.nn.functional.cosine_similarity(query_emb, gallery_emb).item()\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid feature matcher {matcher}\")\n",
    "            n_matches_list[j] = n_matches\n",
    "       \n",
    "        # get the top 5 matches\n",
    "        top_5_matches = np.argsort(n_matches_list)[-5:][::-1]\n",
    "        top_5_matches = [dataset_gallery[_][1] for _ in top_5_matches]\n",
    "\n",
    "        predictions[i] = np.array(top_5_matches)\n",
    "        gt[i] = dataset_query[i][1]\n",
    "\n",
    "    return predictions, gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(preds: np.ndarray, gt: np.ndarray) -> None:\n",
    "    \"\"\"\n",
    "    Compute top-1, top-3, and top-5 accuracy, precision, and recall.\n",
    "    \n",
    "    Args:\n",
    "        preds (np.ndarray): (N, 5) Predictions.\n",
    "        gt (np.ndarray): (N,) Ground truth.\n",
    "    \"\"\"\n",
    "\n",
    "    # compute top-k accuracy\n",
    "    top_1 = np.sum(preds[:, 0] == gt) / len(gt)\n",
    "    top_3 = np.sum([gt[i] in preds[i, :3] for i in range(len(gt))]) / len(gt)\n",
    "    top_5 = np.sum([gt[i] in preds[i, :] for i in range(len(gt))]) / len(gt)\n",
    "\n",
    "    # compute precision and recall for k=1, 3, 5\n",
    "    TP = np.sum( preds[:, 0] == gt )\n",
    "    FP = np.sum( preds[:, 0] != gt )\n",
    "    FN = len(gt) - TP\n",
    "    precision = TP / (TP + FP)\n",
    "    recall = TP / (TP + FN)\n",
    "\n",
    "    print(\"------k=1------\")\n",
    "    print(f\"Top-1 Accuracy: {100*top_1:.2f}%\")\n",
    "    # print(f\"Precision: {100*precision:.2f}%\")\n",
    "    # print(f\"Recall: {100*recall:.2f}%\")\n",
    "    # print(F\"TP: {TP}, FP: {FP}, FN: {FN}\")\n",
    "    print(f\"Correct Matches: {TP}, Incorrect Matches: {FP}\")\n",
    "\n",
    "    # k = 3\n",
    "    TP = np.sum([gt[i] in preds[i, :3] for i in range(len(gt))])\n",
    "    FP = np.sum([gt[i] not in preds[i, :3] for i in range(len(gt))])\n",
    "    FN = len(gt) - TP\n",
    "    precision = TP / (TP + FP)\n",
    "    recall = TP / (TP + FN)\n",
    "\n",
    "    print(\"------k=3------\")\n",
    "    print(f\"Top-3 Accuracy: {100*top_3:.2f}%\")\n",
    "    # print(f\"Precision: {100*precision:.2f}%\")\n",
    "    # print(f\"Recall: {100*recall:.2f}%\")\n",
    "    # print(F\"TP: {TP}, FP: {FP}, FN: {FN}\")\n",
    "    print(f\"Correct Matches: {TP}, Incorrect Matches: {FP}\")\n",
    "\n",
    "    # k = 5\n",
    "    TP = np.sum([gt[i] in preds[i, :] for i in range(len(gt))])\n",
    "    FP = np.sum([gt[i] not in preds[i, :] for i in range(len(gt))])\n",
    "    FN = len(gt) - TP\n",
    "    precision = TP / (TP + FP)\n",
    "    recall = TP / (TP + FN)\n",
    "\n",
    "    print(\"------k=5------\")\n",
    "    print(f\"Top-5 Accuracy: {100*top_5:.2f}%\")\n",
    "    # print(f\"Precision: {100*precision:.2f}%\")\n",
    "    # print(f\"Recall: {100*recall:.2f}%\")\n",
    "    # print(f\"TP: {TP}, FP: {FP}, FN: {FN}\")\n",
    "    print(f\"Correct Matches: {TP}, Incorrect Matches: {FP}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"WII\"\n",
    "detector = \"superpoint\"\n",
    "matcher = \"lightglue\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 3405\n",
      "Number of validation samples: 3498\n",
      "Number of test samples: 3498\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "df_train, df_val, df_test, root = get_dataset(dataset)\n",
    "print(f\"Number of training samples: {len(df_train)}\")\n",
    "print(f\"Number of validation samples: {len(df_val)}\")\n",
    "print(f\"Number of test samples: {len(df_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130\n",
      "78\n",
      "78\n"
     ]
    }
   ],
   "source": [
    "print(len(df_train['identity'].unique()))\n",
    "print(len(df_val['identity'].unique()))\n",
    "print(len(df_test['identity'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>identity</th>\n",
       "      <th>path</th>\n",
       "      <th>bbox</th>\n",
       "      <th>split</th>\n",
       "      <th>gallery_split</th>\n",
       "      <th>flank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>31</td>\n",
       "      <td>257</td>\n",
       "      <td>wii.coco/images/site_0001_T10_01038.jpg</td>\n",
       "      <td>[474.8544, 912.824, 1691.7984, 919.9872]</td>\n",
       "      <td>test</td>\n",
       "      <td>gallery</td>\n",
       "      <td>right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>32</td>\n",
       "      <td>257</td>\n",
       "      <td>wii.coco/images/site_0001_T10_01039.jpg</td>\n",
       "      <td>[339.968, 609.895, 974.848, 474.903]</td>\n",
       "      <td>test</td>\n",
       "      <td>gallery</td>\n",
       "      <td>right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>33</td>\n",
       "      <td>257</td>\n",
       "      <td>wii.coco/images/site_0001_T10_01040.jpg</td>\n",
       "      <td>[578.9696, 590.876, 514.8672, 282.997]</td>\n",
       "      <td>test</td>\n",
       "      <td>query</td>\n",
       "      <td>right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>34</td>\n",
       "      <td>257</td>\n",
       "      <td>wii.coco/images/site_0001_T10_01041.jpg</td>\n",
       "      <td>[212.992, 739.882, 1157.9392, 562.991]</td>\n",
       "      <td>test</td>\n",
       "      <td>query</td>\n",
       "      <td>right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>35</td>\n",
       "      <td>257</td>\n",
       "      <td>wii.coco/images/site_0001_T10_01042.jpg</td>\n",
       "      <td>[115.99872, 443.872, 1243.9552, 686.972]</td>\n",
       "      <td>test</td>\n",
       "      <td>gallery</td>\n",
       "      <td>right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7527</th>\n",
       "      <td>7613</td>\n",
       "      <td>255</td>\n",
       "      <td>wii.coco/images/site_0012_K_T-9_F_01420.jpg</td>\n",
       "      <td>[1285.9392, 684.945, 734.8224, 476.95739999999...</td>\n",
       "      <td>test</td>\n",
       "      <td>gallery</td>\n",
       "      <td>right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7528</th>\n",
       "      <td>7614</td>\n",
       "      <td>255</td>\n",
       "      <td>wii.coco/images/site_0012_K_T-9_F_01421.jpg</td>\n",
       "      <td>[1402.88, 701.8845, 644.9152, 422.8983]</td>\n",
       "      <td>test</td>\n",
       "      <td>gallery</td>\n",
       "      <td>right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7529</th>\n",
       "      <td>7615</td>\n",
       "      <td>255</td>\n",
       "      <td>wii.coco/images/site_0012_K_T-9_F_01422.jpg</td>\n",
       "      <td>[272.79359999999997, 373.8912, 2092.8384, 884....</td>\n",
       "      <td>test</td>\n",
       "      <td>query</td>\n",
       "      <td>right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7530</th>\n",
       "      <td>7616</td>\n",
       "      <td>255</td>\n",
       "      <td>wii.coco/images/site_0012_K_T-9_F_01423.jpg</td>\n",
       "      <td>[494.912, 355.9424, 1871.904, 903.8864]</td>\n",
       "      <td>test</td>\n",
       "      <td>query</td>\n",
       "      <td>right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7531</th>\n",
       "      <td>7617</td>\n",
       "      <td>255</td>\n",
       "      <td>wii.coco/images/site_0012_K_T-9_F_01424.jpg</td>\n",
       "      <td>[664.9344, 336.9824, 1695.9615999999999, 923.9...</td>\n",
       "      <td>test</td>\n",
       "      <td>gallery</td>\n",
       "      <td>right</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3498 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      image_id  identity                                         path  \\\n",
       "22          31       257      wii.coco/images/site_0001_T10_01038.jpg   \n",
       "23          32       257      wii.coco/images/site_0001_T10_01039.jpg   \n",
       "24          33       257      wii.coco/images/site_0001_T10_01040.jpg   \n",
       "25          34       257      wii.coco/images/site_0001_T10_01041.jpg   \n",
       "26          35       257      wii.coco/images/site_0001_T10_01042.jpg   \n",
       "...        ...       ...                                          ...   \n",
       "7527      7613       255  wii.coco/images/site_0012_K_T-9_F_01420.jpg   \n",
       "7528      7614       255  wii.coco/images/site_0012_K_T-9_F_01421.jpg   \n",
       "7529      7615       255  wii.coco/images/site_0012_K_T-9_F_01422.jpg   \n",
       "7530      7616       255  wii.coco/images/site_0012_K_T-9_F_01423.jpg   \n",
       "7531      7617       255  wii.coco/images/site_0012_K_T-9_F_01424.jpg   \n",
       "\n",
       "                                                   bbox split gallery_split  \\\n",
       "22             [474.8544, 912.824, 1691.7984, 919.9872]  test       gallery   \n",
       "23                 [339.968, 609.895, 974.848, 474.903]  test       gallery   \n",
       "24               [578.9696, 590.876, 514.8672, 282.997]  test         query   \n",
       "25               [212.992, 739.882, 1157.9392, 562.991]  test         query   \n",
       "26             [115.99872, 443.872, 1243.9552, 686.972]  test       gallery   \n",
       "...                                                 ...   ...           ...   \n",
       "7527  [1285.9392, 684.945, 734.8224, 476.95739999999...  test       gallery   \n",
       "7528            [1402.88, 701.8845, 644.9152, 422.8983]  test       gallery   \n",
       "7529  [272.79359999999997, 373.8912, 2092.8384, 884....  test         query   \n",
       "7530            [494.912, 355.9424, 1871.904, 903.8864]  test         query   \n",
       "7531  [664.9344, 336.9824, 1695.9615999999999, 923.9...  test       gallery   \n",
       "\n",
       "      flank  \n",
       "22    right  \n",
       "23    right  \n",
       "24    right  \n",
       "25    right  \n",
       "26    right  \n",
       "...     ...  \n",
       "7527  right  \n",
       "7528  right  \n",
       "7529  right  \n",
       "7530  right  \n",
       "7531  right  \n",
       "\n",
       "[3498 rows x 7 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating query and gallery datasets...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 19\u001b[0m\n\u001b[1;32m      8\u001b[0m     transforms \u001b[38;5;241m=\u001b[39m T\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m      9\u001b[0m         T\u001b[38;5;241m.\u001b[39mResize([\u001b[38;5;241m384\u001b[39m, \u001b[38;5;241m384\u001b[39m]),\n\u001b[1;32m     10\u001b[0m         T\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m         )\n\u001b[1;32m     15\u001b[0m     ])\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating query and gallery datasets...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 19\u001b[0m dataset_query, dataset_gallery \u001b[38;5;241m=\u001b[39m get_query_gallery_split(df_val, root, transforms, dataset_name\u001b[38;5;241m=\u001b[39mdataset)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of query samples: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(dataset_query)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of gallery samples: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(dataset_gallery)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "if matcher == \"arcface\" or detector == \"arcface\":\n",
    "    transforms = T.Compose([\n",
    "        T.Resize([112, 112], antialias=True),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "else:\n",
    "    transforms = T.Compose([\n",
    "        T.Resize([384, 384]),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(\n",
    "            mean=[0.5, 0.5, 0.5],\n",
    "            std=[0.5, 0.5, 0.5]\n",
    "        )\n",
    "    ])\n",
    "    \n",
    "\n",
    "print(\"Creating query and gallery datasets...\")\n",
    "dataset_query, dataset_gallery = get_query_gallery_split(df_val, root, transforms, dataset_name=dataset)\n",
    "print(f\"Number of query samples: {len(dataset_query)}\")\n",
    "print(f\"Number of gallery samples: {len(dataset_gallery)}\")\n",
    "\n",
    "preds, gt = run_inference(dataset_query, dataset_gallery, device, detector=detector, matcher=matcher)\n",
    "\n",
    "compute_metrics(preds, gt) # arcface + arcface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of left query samples: 1233\n",
      "Number of left gallery samples: 465\n",
      "Number of right query samples: 1334\n",
      "Number of right gallery samples: 466\n"
     ]
    }
   ],
   "source": [
    "left_query, left_gallery, right_query, right_gallery = get_query_gallery_split(df_val, root, transforms, dataset_name=dataset)\n",
    "print(f\"Number of left query samples: {len(left_query)}\")\n",
    "print(f\"Number of left gallery samples: {len(left_gallery)}\")\n",
    "print(f\"Number of right query samples: {len(right_query)}\")\n",
    "print(f\"Number of right gallery samples: {len(right_gallery)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Memoizing gallery:   0%|          | 2/465 [00:00<00:26, 17.37it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Memoizing gallery: 100%|██████████| 465/465 [00:34<00:00, 13.47it/s]\n",
      "Memoizing query: 100%|██████████| 1233/1233 [01:30<00:00, 13.68it/s]\n",
      "Querying: 100%|██████████| 1233/1233 [2:38:21<00:00,  7.71s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------k=1------\n",
      "Top-1 Accuracy: 63.75%\n",
      "Correct Matches: 786, Incorrect Matches: 447\n",
      "------k=3------\n",
      "Top-3 Accuracy: 69.91%\n",
      "Correct Matches: 862, Incorrect Matches: 371\n",
      "------k=5------\n",
      "Top-5 Accuracy: 71.94%\n",
      "Correct Matches: 887, Incorrect Matches: 346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "preds_left, gt_left = run_inference(left_query, left_gallery, device, detector=detector, matcher=matcher, memo_gallery=[], memo_query=[])\n",
    "compute_metrics(preds_left, gt_left) # arcface + arcface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Memoizing gallery:   0%|          | 2/466 [00:00<00:35, 12.90it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Memoizing gallery: 100%|██████████| 466/466 [00:34<00:00, 13.64it/s]\n",
      "Memoizing query: 100%|██████████| 1334/1334 [01:34<00:00, 14.12it/s]\n",
      "Querying:  55%|█████▌    | 737/1334 [1:36:13<1:19:41,  8.01s/it]"
     ]
    }
   ],
   "source": [
    "preds_right, gt_right = run_inference(right_query, right_gallery, device, detector=detector, matcher=matcher, memo_gallery=[], memo_query=[])\n",
    "compute_metrics(preds_right, gt_right) # arcface + arcface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the left and right predictions\n",
    "preds = np.concatenate([preds_left, preds_right])\n",
    "gt = np.concatenate([gt_left, gt_right])\n",
    "compute_metrics(preds, gt) # arcface + arcface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the numpy arrays\n",
    "np.save(\"preds_left.npy\", preds_left)\n",
    "np.save(\"gt_left.npy\", gt_left)\n",
    "np.save(\"preds_right.npy\", preds_right)\n",
    "np.save(\"gt_right.npy\", gt_right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 42, GT: 1.0, Preds: [6. 7. 4. 4. 8.]\n",
      "Index: 43, GT: 1.0, Preds: [2. 1. 6. 6. 1.]\n",
      "Index: 44, GT: 1.0, Preds: [4. 8. 7. 9. 9.]\n",
      "Index: 45, GT: 1.0, Preds: [4. 8. 4. 9. 9.]\n",
      "Index: 46, GT: 1.0, Preds: [6. 6. 4. 4. 8.]\n",
      "Index: 47, GT: 1.0, Preds: [6. 0. 6. 6. 3.]\n",
      "Index: 48, GT: 1.0, Preds: [6. 4. 4. 3. 6.]\n",
      "Index: 50, GT: 4.0, Preds: [5. 4. 1. 4. 0.]\n",
      "Index: 57, GT: 5.0, Preds: [3. 6. 6. 6. 5.]\n",
      "Index: 71, GT: 6.0, Preds: [2. 0. 7. 1. 7.]\n",
      "Index: 77, GT: 7.0, Preds: [0. 9. 5. 7. 4.]\n",
      "Index: 81, GT: 7.0, Preds: [4. 2. 2. 0. 9.]\n",
      "Index: 94, GT: 8.0, Preds: [4. 4. 2. 7. 6.]\n",
      "Index: 98, GT: 9.0, Preds: [4. 5. 2. 0. 2.]\n",
      "Index: 99, GT: 9.0, Preds: [4. 4. 4. 8. 8.]\n",
      "Index: 101, GT: 9.0, Preds: [8. 4. 2. 4. 3.]\n",
      "Index: 102, GT: 9.0, Preds: [6. 6. 6. 3. 6.]\n",
      "Index: 103, GT: 9.0, Preds: [7. 7. 1. 0. 6.]\n",
      "Index: 104, GT: 9.0, Preds: [5. 9. 2. 0. 5.]\n",
      "Index: 105, GT: 9.0, Preds: [2. 0. 5. 9. 4.]\n",
      "Index: 109, GT: 9.0, Preds: [5. 0. 9. 9. 8.]\n"
     ]
    }
   ],
   "source": [
    "# print the index at which gt and preds differ\n",
    "for i in range(len(gt)):\n",
    "    if gt[i] != preds[i, 0]:\n",
    "        print(f\"Index: {i}, GT: {gt[i]}, Preds: {preds[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------k=1------\n",
      "Top-1 Accuracy: 80.91%\n",
      "Correct Matches: 89, Incorrect Matches: 21\n",
      "------k=3------\n",
      "Top-3 Accuracy: 84.55%\n",
      "Correct Matches: 93, Incorrect Matches: 17\n",
      "------k=5------\n",
      "Top-5 Accuracy: 87.27%\n",
      "Correct Matches: 96, Incorrect Matches: 14\n"
     ]
    }
   ],
   "source": [
    "compute_metrics(preds, gt) # superpoint + lightglue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wildlife10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
